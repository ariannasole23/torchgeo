{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchgeo.datasets import VHR10\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchgeo.trainers import InstanceSegmentationTask  \n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DataLoader.\"\"\"\n",
    "    max_height = max(sample['image'].shape[1] for sample in batch)\n",
    "    max_width = max(sample['image'].shape[2] for sample in batch)\n",
    "\n",
    "    images = torch.stack([\n",
    "        F.pad(sample['image'], (0, max_width - sample['image'].shape[2], 0, max_height - sample['image'].shape[1]))\n",
    "        for sample in batch\n",
    "    ])\n",
    "\n",
    "    targets = [\n",
    "        {\n",
    "            \"labels\": sample[\"labels\"].to(torch.int64),\n",
    "            \"boxes\": sample[\"boxes\"].to(torch.float32),\n",
    "            \"masks\": F.pad(\n",
    "                sample[\"masks\"],\n",
    "                (0, max_width - sample[\"masks\"].shape[2], 0, max_height - sample[\"masks\"].shape[1]),\n",
    "            ).to(torch.uint8),\n",
    "        }\n",
    "        for sample in batch\n",
    "    ]\n",
    "\n",
    "    return {\"image\": images, \"target\": targets}\n",
    "\n",
    "def visualize_predictions(image, predictions, targets):\n",
    "    \"\"\"Visualize predictions and ground truth.\"\"\"\n",
    "    image = to_pil_image(image)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Predictions\n",
    "    for box, label in zip(predictions['boxes'], predictions['labels']):\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\"Pred: {label.item()}\", color='red', fontsize=12)\n",
    "\n",
    "    # Ground truth\n",
    "    for box, label in zip(targets['boxes'], targets['labels']):\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\"GT: {label.item()}\", color='blue', fontsize=12)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"Plot training and validation losses over epochs.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize VHR-10 dataset\n",
    "train_dataset = VHR10(root=\"data\", split=\"positive\", transforms=None, download=True)\n",
    "val_dataset = VHR10(root=\"data\", split=\"positive\", transforms=None)\n",
    "\n",
    "# Subset for quick experimentation (adjust N as needed)\n",
    "N = 100\n",
    "train_subset = Subset(train_dataset, list(range(N)))\n",
    "val_subset = Subset(val_dataset, list(range(N)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_subset, batch_size=8, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "\n",
    "    # Trainer setup\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5, \n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1\n",
    "    )\n",
    "\n",
    "    task = InstanceSegmentationTask(\n",
    "        model=\"mask_rcnn\",          \n",
    "        backbone=\"resnet50\",        \n",
    "        weights=\"imagenet\",         # Pretrained on ImageNet\n",
    "        num_classes=11,             # VHR-10 has 10 classes + 1 background\n",
    "        lr=1e-3,                    \n",
    "        freeze_backbone=False       \n",
    "    )\n",
    "\n",
    "    print('\\nSTART TRAINING\\n')\n",
    "    # trainer.fit(task, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(5):\n",
    "        trainer.fit(task, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "        train_loss = task.trainer.callback_metrics.get(\"train_loss\")\n",
    "        val_loss = task.trainer.callback_metrics.get(\"val_loss\")\n",
    "        if train_loss is not None:\n",
    "            train_losses.append(train_loss.item())\n",
    "        if val_loss is not None:\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    plot_losses(train_losses, val_losses)\n",
    "\n",
    "    #trainer.test(task, dataloaders=val_loader)\n",
    "\n",
    "    # Inference and Visualization\n",
    "    sample = train_dataset[1]\n",
    "    image = sample['image'].unsqueeze(0)  \n",
    "    predictions = task.predict_step({\"image\": image}, batch_idx=0)\n",
    "    visualize_predictions(image[0], predictions[0], sample)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
