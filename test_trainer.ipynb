{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "S0al7K8Fc0Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQBpL3DTHh2v",
        "outputId": "0f96c780-21d7-42fe-fc97-db77155f826c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lightning'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-830d39adcfdd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVHR10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_pil_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip install torch torchvision torchgeo lightning matplotlib\n",
        "\n",
        "import torch\n",
        "import lightning.pytorch as pl\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchgeo.datasets import VHR10\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchgeo.trainers import InstanceSegmentationTask\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for DataLoader.\"\"\"\n",
        "    max_height = max(sample['image'].shape[1] for sample in batch)\n",
        "    max_width = max(sample['image'].shape[2] for sample in batch)\n",
        "\n",
        "    images = torch.stack([\n",
        "        F.pad(sample['image'], (0, max_width - sample['image'].shape[2], 0, max_height - sample['image'].shape[1]))\n",
        "        for sample in batch\n",
        "    ])\n",
        "\n",
        "    targets = [\n",
        "        {\n",
        "            \"labels\": sample[\"labels\"].to(torch.int64),\n",
        "            \"boxes\": sample[\"boxes\"].to(torch.float32),\n",
        "            \"masks\": F.pad(\n",
        "                sample[\"masks\"],\n",
        "                (0, max_width - sample[\"masks\"].shape[2], 0, max_height - sample[\"masks\"].shape[1]),\n",
        "            ).to(torch.uint8),\n",
        "        }\n",
        "        for sample in batch\n",
        "    ]\n",
        "\n",
        "    return {\"image\": images, \"target\": targets}\n",
        "\n",
        "def visualize_predictions(image, predictions, targets):\n",
        "    \"\"\"Visualize predictions and ground truth.\"\"\"\n",
        "    image = to_pil_image(image)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "    ax.imshow(image)\n",
        "\n",
        "    # Predictions\n",
        "    for box, label in zip(predictions['boxes'], predictions['labels']):\n",
        "        x1, y1, x2, y2 = box\n",
        "        rect = Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1, f\"Pred: {label.item()}\", color='red', fontsize=12)\n",
        "\n",
        "    # Ground truth\n",
        "    for box, label in zip(targets['boxes'], targets['labels']):\n",
        "        x1, y1, x2, y2 = box\n",
        "        rect = Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='blue', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1, f\"GT: {label.item()}\", color='blue', fontsize=12)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_losses(train_losses, val_losses):\n",
        "    \"\"\"Plot training and validation losses over epochs.\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', marker='o')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', marker='s')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Initialize VHR-10 dataset\n",
        "train_dataset = VHR10(root=\"data\", split=\"positive\", transforms=None, download=True)\n",
        "val_dataset = VHR10(root=\"data\", split=\"positive\", transforms=None)\n",
        "\n",
        "# Subset for quick experimentation (adjust N as needed)\n",
        "N = 100\n",
        "train_subset = Subset(train_dataset, list(range(N)))\n",
        "val_subset = Subset(val_dataset, list(range(N)))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import multiprocessing\n",
        "    multiprocessing.set_start_method('spawn', force=True)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_subset, batch_size=8, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
        "\n",
        "    # Trainer setup\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=5,\n",
        "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "        devices=1\n",
        "    )\n",
        "\n",
        "    task = InstanceSegmentationTask(\n",
        "        model=\"mask_rcnn\",\n",
        "        backbone=\"resnet50\",\n",
        "        weights=\"imagenet\",         # Pretrained on ImageNet\n",
        "        num_classes=11,             # VHR-10 has 10 classes + 1 background\n",
        "        lr=1e-3,\n",
        "        freeze_backbone=False\n",
        "    )\n",
        "\n",
        "    print('\\nSTART TRAINING\\n')\n",
        "    # trainer.fit(task, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "    train_losses, val_losses = [], []\n",
        "    for epoch in range(5):\n",
        "        trainer.fit(task, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "        train_loss = task.trainer.callback_metrics.get(\"train_loss\")\n",
        "        val_loss = task.trainer.callback_metrics.get(\"val_loss\")\n",
        "        if train_loss is not None:\n",
        "            train_losses.append(train_loss.item())\n",
        "        if val_loss is not None:\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    plot_losses(train_losses, val_losses)\n",
        "\n",
        "    #trainer.test(task, dataloaders=val_loader)\n",
        "\n",
        "    # Inference and Visualization\n",
        "    sample = train_dataset[1]\n",
        "    image = sample['image'].unsqueeze(0)\n",
        "    predictions = task.predict_step({\"image\": image}, batch_idx=0)\n",
        "    visualize_predictions(image[0], predictions[0], sample)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}